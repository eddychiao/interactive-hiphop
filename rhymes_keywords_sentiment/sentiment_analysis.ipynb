{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.corpus import stopwords \n",
    "\n",
    "from pandas import read_excel\n",
    "\n",
    "data_path = './sentiment_analysis_data'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Hu & Liu Lexicon\n",
    "This cell gets lexicons with a simple flavor, pulling in about 6,800 positive and negative words from a study by Minqing Hu and Bing Liu."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiments = ['Positive', 'Negative']\n",
    "sentiment_path_map = {\n",
    "    sentiments[0]: data_path + '/hu_positive_lexicon.txt',\n",
    "    sentiments[1]: data_path + '/hu_negative_lexicon.txt',\n",
    "}\n",
    "sentiment_lexicon_map = {s: set() for s in sentiments}\n",
    "\n",
    "for sentiment in sentiments:\n",
    "    path = sentiment_path_map[sentiment]\n",
    "    lexicon = sentiment_lexicon_map[sentiment]\n",
    "    with open(path) as file:\n",
    "        for line in file:\n",
    "            word = line.rstrip()\n",
    "            lexicon.add(word)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Harvard Lexicon\n",
    "Running this cell is a good bit spicier. We get nearly 12,000 words and a number of more complex sentiments related to them. Mega long list proceeds of Harvard 2000; RIP Philip Stone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "sentiments = ['Positiv', 'Negativ', 'Strong', 'Power', 'Weak', 'Submit', 'Active', 'Passive', 'Pleasur', 'Pain', 'Feel', 'Arousal', 'EMOT', 'Virtue', 'Vice', 'Ovrst', 'Undrst']\n",
    "sentiment_lexicon_map = {s: set() for s in sentiments}\n",
    "\n",
    "lexicon_df = read_excel(data_path + '/harvard_lexicon.xls')\n",
    "\n",
    "for _, row in lexicon_df.iterrows():\n",
    "    word = str(row['Entry']).lower()\n",
    "    if not word.isalpha():\n",
    "        continue\n",
    "    for sentiment in sentiments:\n",
    "        if row[sentiment] == sentiment:\n",
    "            lexicon = sentiment_lexicon_map[sentiment]\n",
    "            lexicon.add(word)\n",
    "            \n",
    "# sentiment_lexicon_map"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Import and clean song lyrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['proceed',\n",
       " 'give',\n",
       " 'need',\n",
       " 'uh',\n",
       " 'motherfuckers',\n",
       " 'get',\n",
       " 'live',\n",
       " 'motherfuckers',\n",
       " 'proceed',\n",
       " 'give',\n",
       " 'need',\n",
       " 'motherfuckers',\n",
       " 'get',\n",
       " 'live',\n",
       " 'motherfuckers',\n",
       " 'turn',\n",
       " 'mics',\n",
       " 'proceed',\n",
       " 'give',\n",
       " 'need',\n",
       " 'turn',\n",
       " 'mic',\n",
       " 'yeah',\n",
       " 'beat',\n",
       " 'knocking',\n",
       " 'need',\n",
       " 'mic',\n",
       " 'though',\n",
       " 'turn',\n",
       " 'shit',\n",
       " 'fuck',\n",
       " 'east',\n",
       " 'coast',\n",
       " 'motherfuckers',\n",
       " 'uh',\n",
       " 'bad',\n",
       " 'boy',\n",
       " 'motherfuckers',\n",
       " 'turn',\n",
       " 'louder',\n",
       " 'yeah',\n",
       " 'uh',\n",
       " 'proceed',\n",
       " 'give',\n",
       " 'need',\n",
       " 'motherfuckers',\n",
       " 'motherfuckers',\n",
       " 'uh',\n",
       " 'motherfuckers',\n",
       " 'uh',\n",
       " 'verse',\n",
       " 'notorious',\n",
       " 'shot',\n",
       " 'ya',\n",
       " 'separate',\n",
       " 'weak',\n",
       " 'obsolete',\n",
       " 'hard',\n",
       " 'creep',\n",
       " 'brooklyn',\n",
       " 'streets',\n",
       " 'nigga',\n",
       " 'fuck',\n",
       " 'bickerin',\n",
       " 'beef',\n",
       " 'hear',\n",
       " 'sweat',\n",
       " 'tricklin',\n",
       " 'cheek',\n",
       " 'heartbeat',\n",
       " 'sound',\n",
       " 'like',\n",
       " 'sasquatch',\n",
       " 'feet',\n",
       " 'thunderin',\n",
       " 'shakin',\n",
       " 'concrete',\n",
       " 'shit',\n",
       " 'stop',\n",
       " 'foil',\n",
       " 'plot',\n",
       " 'neighbors',\n",
       " 'call',\n",
       " 'cops',\n",
       " 'said',\n",
       " 'heard',\n",
       " 'mad',\n",
       " 'shots',\n",
       " 'saw',\n",
       " 'drop',\n",
       " 'three',\n",
       " 'quarter',\n",
       " 'slaughter',\n",
       " 'electrical',\n",
       " 'tape',\n",
       " 'around',\n",
       " 'daughter',\n",
       " 'old',\n",
       " 'school',\n",
       " 'new',\n",
       " 'school',\n",
       " 'need',\n",
       " 'learn',\n",
       " 'though',\n",
       " 'burn',\n",
       " 'baby',\n",
       " 'burn',\n",
       " 'like',\n",
       " 'disco',\n",
       " 'inferno',\n",
       " 'burn',\n",
       " 'slow',\n",
       " 'like',\n",
       " 'blunts',\n",
       " 'yayo',\n",
       " 'peel',\n",
       " 'skins',\n",
       " 'idaho',\n",
       " 'potato',\n",
       " 'niggas',\n",
       " 'know',\n",
       " 'lyrical',\n",
       " 'molesting',\n",
       " 'taking',\n",
       " 'place',\n",
       " 'fucking',\n",
       " 'ai',\n",
       " 'safe',\n",
       " 'make',\n",
       " 'skin',\n",
       " 'chafe',\n",
       " 'rashes',\n",
       " 'asses',\n",
       " 'bumps',\n",
       " 'bruises',\n",
       " 'blunts',\n",
       " 'land',\n",
       " 'cruisers',\n",
       " 'big',\n",
       " 'poppa',\n",
       " 'smash',\n",
       " 'fools',\n",
       " 'bash',\n",
       " 'fools',\n",
       " 'niggas',\n",
       " 'mad',\n",
       " 'know',\n",
       " 'cash',\n",
       " 'rules',\n",
       " 'everything',\n",
       " 'around',\n",
       " 'two',\n",
       " 'glock',\n",
       " 'nines',\n",
       " 'motherfucker',\n",
       " 'whispering',\n",
       " 'mines',\n",
       " 'crooklyn',\n",
       " 'finest',\n",
       " 'crooklyn',\n",
       " 'finest',\n",
       " 'rewind',\n",
       " 'bad',\n",
       " 'boy',\n",
       " 'behind',\n",
       " 'bad',\n",
       " 'boy',\n",
       " 'behind']"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "song = ''\n",
    "path = data_path + \"/who_shot_ya.txt\"\n",
    "# path = data_path + \"/juicy.txt\"\n",
    "with open(path, \"r\") as f:\n",
    "    for line in f:\n",
    "        line = line.rstrip().lower()\n",
    "        song += line + ' '\n",
    "\n",
    "# Remove stop words and non-alphabetical tokens\n",
    "stop_words = set(stopwords.words('english'))\n",
    "extra_stop_words = [\"n't\", \"'s\", \"'m\", \"``\", \"'\", '\"', '.', \",\"]\n",
    "for word in extra_stop_words:\n",
    "    stop_words.add(word)\n",
    "song_words = word_tokenize(song)\n",
    "song_words = [w for w in song_words if w not in stop_words and w.isalpha()]\n",
    "\n",
    "song_words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Bag of Words Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'Positive': 7, 'Negative': 27}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scores = {s: 0 for s in sentiments}\n",
    "\n",
    "for word in song_words:\n",
    "    for sentiment in sentiments:\n",
    "        lexicon = sentiment_lexicon_map[sentiment]\n",
    "        if word in lexicon:\n",
    "            scores[sentiment] += 1\n",
    "            \n",
    "scaled_scores = {key: val / len(song_words) for key, val in scores.items()}\n",
    "            \n",
    "scores"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
